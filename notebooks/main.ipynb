{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "src_path = Path.cwd().parent / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_data\n",
    "\n",
    "# Set up data paths\n",
    "data_path = Path.cwd().parent / \"data\" / \"ag_news\"\n",
    "train_path = data_path / \"train.csv\"\n",
    "test_path = data_path / \"test.csv\"\n",
    "\n",
    "# Load training data\n",
    "df_train = load_data(train_path)\n",
    "X_train = (df_train[\"Title\"] + \" \" + df_train[\"Description\"]).values\n",
    "y_train = df_train[\"Class Index\"].values\n",
    "\n",
    "# Load test data\n",
    "df_test = load_data(test_path)\n",
    "X_test = (df_test[\"Title\"] + \" \" + df_test[\"Description\"]).values\n",
    "y_test = df_test[\"Class Index\"].values\n",
    "\n",
    "# Show training data\n",
    "# df_train.head()\n",
    "\n",
    "# Show test data\n",
    "# df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import vectorize_data_fit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorize data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vect = vectorize_data_fit(vectorizer, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_trainer import train_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "lg_model = LogisticRegression()\n",
    "#lg_trained_model = train_model(lg_model, X_train_vect, y_train)\n",
    "\n",
    "# Train Naive Bayes Model\n",
    "nb_model = MultinomialNB()\n",
    "nb_trained_model = train_model(nb_model, X_train_vect, y_train)\n",
    "\n",
    "# Train Random Forest Model\n",
    "rf_model = RandomForestClassifier()\n",
    "#rf_trained_model = train_model(rf_model, X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from perturbation import apply_perturbation\n",
    "\n",
    "# # Save perturbed data to file\n",
    "\n",
    "# X_data = X_test[:1000]\n",
    "# level = 1.0\n",
    "\n",
    "# perturbed_data = apply_perturbation(\n",
    "#     X_data,\n",
    "#     level,\n",
    "#     save_path=f\"perturbed_data/deletion/perturbed_data_{level:.2f}.pkl\"\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'flair' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_robustness\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Evaluation pipeline\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Number of samples to evaluate\u001b[39;00m\n\u001b[0;32m      6\u001b[0m X_sample \u001b[38;5;241m=\u001b[39m X_test[:\u001b[38;5;241m1000\u001b[39m]\n",
      "File \u001b[1;32mc:\\Programvaruteknik\\DT133G - Självständigt arbete\\source\\ml-robustness-monitor\\src\\evaluator.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mperturbation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_perturbation\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvectorizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vectorize_data\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     mean_corruption_error,\n\u001b[0;32m     10\u001b[0m     relative_corruption_error,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     our_metric\n\u001b[0;32m     14\u001b[0m ) \n",
      "File \u001b[1;32mc:\\Programvaruteknik\\DT133G - Självständigt arbete\\source\\ml-robustness-monitor\\src\\perturbation.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetAugmenter, CharSwapAugmenter, EasyDataAugmenter, CLAREAugmenter, Augmenter, DeletionAugmenter, BackTranslationAugmenter, EmbeddingAugmenter, CheckListAugmenter\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordSwapNeighboringCharacterSwap, WordSwapHomoglyphSwap, WordMergeMaskedLM\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_perturbation\u001b[39m(\n\u001b[0;32m     10\u001b[0m     X: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m     11\u001b[0m     level: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Load previously saved perturbed data (if exists)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\t440p\\miniconda3\\envs\\thesis-py310\\lib\\site-packages\\textattack\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Welcome to the API references for TextAttack!\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mWhat is TextAttack?\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mTextAttack provides components for common NLP tasks like sentence encoding, grammar-checking, and word replacement that can be used on their own.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattack_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttackArgs, CommandLineAttackArgs\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugment_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AugmenterArgs\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetArgs\n",
      "File \u001b[1;32mc:\\Users\\t440p\\miniconda3\\envs\\thesis-py310\\lib\\site-packages\\textattack\\attack_args.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextattack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARGS_SPLIT_TOKEN, load_module_from_file\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Attack\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetArgs\n",
      "File \u001b[1;32mc:\\Users\\t440p\\miniconda3\\envs\\thesis-py310\\lib\\site-packages\\textattack\\shared\\__init__.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mShared TextAttack Functions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m=============================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validators\n",
      "File \u001b[1;32mc:\\Users\\t440p\\miniconda3\\envs\\thesis-py310\\lib\\site-packages\\textattack\\shared\\utils\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstall\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimporting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\t440p\\miniconda3\\envs\\thesis-py310\\lib\\site-packages\\textattack\\shared\\utils\\strings.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflair\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjieba\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimporting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader\n",
      "File \u001b[1;32mc:\\Users\\t440p\\miniconda3\\envs\\thesis-py310\\lib\\site-packages\\flair\\__init__.py:43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# global variable: arrow symbol\u001b[39;00m\n\u001b[0;32m     41\u001b[0m _arrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m → \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: E402 import after setting device\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     data,\n\u001b[0;32m     45\u001b[0m     models,\n\u001b[0;32m     46\u001b[0m     nn,\n\u001b[0;32m     47\u001b[0m     trainers,\n\u001b[0;32m     48\u001b[0m     visual,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m logging\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdictConfig(\n\u001b[0;32m     52\u001b[0m     {\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     }\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     68\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflair\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\t440p\\miniconda3\\envs\\thesis-py310\\lib\\site-packages\\flair\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mentity_linker_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanClassifier\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mentity_mention_linking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EntityMentionLinker\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LanguageModel\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlemmatizer_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lemmatizer\n",
      "File \u001b[1;32mc:\\Users\\t440p\\miniconda3\\envs\\thesis-py310\\lib\\site-packages\\flair\\models\\entity_mention_linking.py:249\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_state\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    243\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_get_state(),\n\u001b[0;32m    244\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlowercase,\n\u001b[0;32m    245\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_punctuation\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_punctuation,\n\u001b[0;32m    246\u001b[0m         }\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAb3PEntityPreprocessor\u001b[39;00m(EntityPreprocessor):\n\u001b[0;32m    250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Entity preprocessor which uses Ab3P, an (biomedical) abbreviation definition detector.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    Abbreviation definition identification based on automatic precision estimates.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    https://github.com/ncbi-nlp/Ab3P.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         preprocessor: Optional[EntityPreprocessor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    261\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\t440p\\miniconda3\\envs\\thesis-py310\\lib\\site-packages\\flair\\models\\entity_mention_linking.py:307\u001b[0m, in \u001b[0;36mAb3PEntityPreprocessor\u001b[1;34m()\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mprocess_entity_name(entity_name)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m entity_name\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_abbreviation_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: \u001b[38;5;28mlist\u001b[39m[\u001b[43mflair\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mSentence]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processes the given sentences with the Ab3P tool.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m    The function returns a (nested) dictionary containing the abbreviations found for each sentence, e.g.:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m        abbreviation_dict: abbreviations and their resolution detected in each input sentence\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    325\u001b[0m     abbreviation_dict: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'flair' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from evaluator import evaluate_robustness\n",
    "\n",
    "# Evaluation pipeline\n",
    "\n",
    "# Number of samples to evaluate\n",
    "X_sample = X_test[:1000]\n",
    "y_sample = y_test[:1000]\n",
    "\n",
    "# Define perturbation levels to test\n",
    "perturbation_levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Add required metrics to evaluate\n",
    "metrics = [\"base_accuracy\", \"robustness_score\", \"effective_robustness\"]\n",
    "\n",
    "# Use filepath when using pertrubed data from files for faster testing\n",
    "file_path = Path(\"perturbed_data/charswap\")\n",
    "\n",
    "results, metrics_summary = evaluate_robustness(\n",
    "    nb_trained_model,\n",
    "    vectorizer,\n",
    "    X_sample, \n",
    "    y_sample,\n",
    "    perturbation_levels,\n",
    "    metrics,\n",
    "    file_path\n",
    ")\n",
    "\n",
    "print(\"*** Metrics Summary ***\")\n",
    "for metric, value in metrics_summary.items():\n",
    "    print(f\"{metric}: {value: .4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"*** Results Per Perturbation Level\")\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
